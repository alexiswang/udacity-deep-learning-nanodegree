{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Seq2SeqDataset, Vocabulary\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data.Seq2SeqDataset at 0x7f50f498c4f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate the custom dataset\n",
    "train_dataset = Seq2SeqDataset()\n",
    "train_dataset.load_train_data()\n",
    "train_dataset.get_pairs()\n",
    "train_dataset.tokenize()\n",
    "# create a vocabulary based on training data\n",
    "vocab = Vocabulary().from_iterator(train_dataset)\n",
    "# apply the vocabulary to numerize the training data\n",
    "train_dataset.numerize(vocab)\n",
    "# creat test dataset and apply the vocabulary to numerize the test data\n",
    "val_dataset = Seq2SeqDataset()\n",
    "val_dataset.load_val_data().get_pairs().tokenize()\n",
    "val_dataset.numerize(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3451\n",
      "5000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "# get a sense of the data\n",
    "print(vocab.word_count)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def padding(batch:list):\n",
    "    # print(batch)\n",
    "    src_seqs = [pairs[0] for pairs in batch]\n",
    "    src_padded = pad_sequence(src_seqs, padding_value=3) # 3 represents 'pad'\n",
    "    trg_seqs = [pairs[1] for pairs in batch]\n",
    "    trg_padded = pad_sequence(trg_seqs, padding_value=3)\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, \n",
    "                        collate_fn=padding\n",
    "                        )\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, \n",
    "                        collate_fn=padding\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 8.101334762573241 || val loss 8.120554367701212\n",
      "epoch 1 train loss 7.975427377223968 || val loss 7.698226531346639\n",
      "epoch 2 train loss 7.7873450994491575 || val loss 7.2554095188776655\n",
      "epoch 3 train loss 7.45891923904419 || val loss 6.4916675090789795\n",
      "epoch 4 train loss 6.859838712215423 || val loss 5.348465879758199\n",
      "epoch 5 train loss 5.870908486843109 || val loss 4.196055233478546\n",
      "epoch 6 train loss 4.7138101875782015 || val loss 3.3295124570528665\n",
      "epoch 7 train loss 3.79291672706604 || val loss 2.7480830748875937\n",
      "epoch 8 train loss 3.1611127495765685 || val loss 2.3885755836963654\n",
      "epoch 9 train loss 2.7499813050031663 || val loss 2.1622140606244407\n",
      "epoch 10 train loss 2.482448935508728 || val loss 2.0034494598706565\n",
      "epoch 11 train loss 2.291770887374878 || val loss 1.884276459614436\n",
      "epoch 12 train loss 2.1550375282764436 || val loss 1.7897451718648274\n",
      "epoch 13 train loss 2.047431567311287 || val loss 1.7171014348665874\n",
      "epoch 14 train loss 1.96210874915123 || val loss 1.657659540573756\n",
      "epoch 15 train loss 1.894885778427124 || val loss 1.6142343481381733\n",
      "epoch 16 train loss 1.8346463322639466 || val loss 1.5817674299081166\n",
      "epoch 17 train loss 1.7847699910402297 || val loss 1.5553553601106007\n",
      "epoch 18 train loss 1.7382902950048447 || val loss 1.5371545255184174\n",
      "epoch 19 train loss 1.7044405162334442 || val loss 1.5257325669129689\n",
      "epoch 20 train loss 1.6770539581775665 || val loss 1.5202393333117168\n",
      "epoch 21 train loss 1.6525694653391838 || val loss 1.5176817278067272\n",
      "epoch 22 train loss 1.6320269837975503 || val loss 1.517587681611379\n",
      "epoch 23 train loss 1.6124695748090745 || val loss 1.51974555850029\n",
      "epoch 24 train loss 1.5950316086411476 || val loss 1.5236393511295319\n",
      "epoch 25 train loss 1.580082055926323 || val loss 1.5282362401485443\n",
      "epoch 26 train loss 1.5645289957523345 || val loss 1.5330897867679596\n",
      "epoch 27 train loss 1.5508922830224037 || val loss 1.5384897987047832\n",
      "epoch 28 train loss 1.538260979950428 || val loss 1.544236918290456\n",
      "epoch 29 train loss 1.5269605979323386 || val loss 1.5502918561299641\n",
      "epoch 30 train loss 1.5164032861590386 || val loss 1.5562785267829895\n",
      "epoch 31 train loss 1.5068541437387466 || val loss 1.5623166064421337\n",
      "epoch 32 train loss 1.4977817803621292 || val loss 1.5687585771083832\n",
      "epoch 33 train loss 1.490310637652874 || val loss 1.5748793284098308\n",
      "epoch 34 train loss 1.4815562412142753 || val loss 1.5820924540360768\n",
      "epoch 35 train loss 1.4755116194486617 || val loss 1.5876375039418538\n",
      "epoch 36 train loss 1.4682143196463584 || val loss 1.592493365208308\n",
      "epoch 37 train loss 1.4617521047592164 || val loss 1.5996045768260956\n",
      "epoch 38 train loss 1.4568004757165909 || val loss 1.6060224274794261\n",
      "epoch 39 train loss 1.4514816418290137 || val loss 1.6117074986298878\n",
      "epoch 40 train loss 1.445852293074131 || val loss 1.6186862389246623\n",
      "epoch 41 train loss 1.4415592342615127 || val loss 1.6236196955045064\n",
      "epoch 42 train loss 1.437055379152298 || val loss 1.6287358502546947\n",
      "epoch 43 train loss 1.4327856034040451 || val loss 1.6354694664478302\n",
      "epoch 44 train loss 1.4290409207344055 || val loss 1.640403578678767\n",
      "epoch 45 train loss 1.4257931098341943 || val loss 1.6464974880218506\n",
      "epoch 46 train loss 1.422131422162056 || val loss 1.6516424516836803\n",
      "epoch 47 train loss 1.4185676887631415 || val loss 1.6577169199784596\n",
      "epoch 48 train loss 1.4155961260199548 || val loss 1.6616866290569305\n",
      "epoch 49 train loss 1.413005346059799 || val loss 1.6665333807468414\n"
     ]
    }
   ],
   "source": [
    "from src.model import Seq2Seq, Encoder, Decoder\n",
    "import torch.nn as nn\n",
    "from train import epoch_train, epoch_evaluate\n",
    "learning_rate = 0.00001\n",
    "hidden_size = 300 # encoder and decoder hidden size\n",
    "embedding_size= 300\n",
    "dropout = 0.5\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "words_count = vocab.word_count\n",
    "\n",
    "encoder = Encoder(input_size=words_count, hidden_size=hidden_size, embedding_size=embedding_size, dropout=dropout)\n",
    "decoder = Decoder(output_size=words_count, hidden_size=hidden_size, embedding_size=embedding_size, dropout=dropout)\n",
    "seq2seq = Seq2Seq(encoder = encoder, decoder=decoder)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = epoch_train(seq2seq, train_dataloader, optimizer=optimizer, criterion=criterion, batch_size=batch_size)\n",
    "    val_loss = epoch_evaluate(seq2seq, val_dataloader, criterion=criterion)\n",
    "    print(f\"epoch {epoch} train loss {train_loss} || val loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
